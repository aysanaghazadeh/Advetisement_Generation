{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a35ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ganimh/miniconda3/envs/layoutgpt/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">17</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 14 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> autocast                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">contextlib</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> contextmanager, nullcontext                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 16 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torchvision</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 17 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">ldm.util</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> instantiate_from_config                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 18 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">ldm.models.diffusion.ddim</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> DDIMSampler                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 19 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">ldm.models.diffusion.plms</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> PLMSSampler                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ModuleNotFoundError: </span>No module named <span style=\"color: #008000; text-decoration-color: #008000\">'ldm'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m17\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 14 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtorch\u001b[0m \u001b[94mimport\u001b[0m autocast                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mcontextlib\u001b[0m \u001b[94mimport\u001b[0m contextmanager, nullcontext                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 16 \u001b[0m\u001b[94mimport\u001b[0m \u001b[4;96mtorchvision\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 17 \u001b[94mfrom\u001b[0m \u001b[4;96mldm\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mutil\u001b[0m \u001b[94mimport\u001b[0m instantiate_from_config                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 18 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mldm\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mmodels\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mdiffusion\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mddim\u001b[0m \u001b[94mimport\u001b[0m DDIMSampler                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 19 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mldm\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mmodels\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mdiffusion\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mplms\u001b[0m \u001b[94mimport\u001b[0m PLMSSampler                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 20 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mModuleNotFoundError: \u001b[0mNo module named \u001b[32m'ldm'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse, os, sys, glob\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "#from imwatermark import WatermarkEncoder\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "import torchvision\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "from transformers import AutoFeatureExtractor\n",
    "import clip\n",
    "from torchvision.transforms import Resize\n",
    "wm = \"Paint-by-Example\"\n",
    "# wm_encoder = WatermarkEncoder()\n",
    "# wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n",
    "safety_model_id = \"CompVis/stable-diffusion-safety-checker\"\n",
    "safety_feature_extractor = AutoFeatureExtractor.from_pretrained(safety_model_id)\n",
    "safety_checker = StableDiffusionSafetyChecker.from_pretrained(safety_model_id)\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "def get_tensor_clip(normalize=True, toTensor=True):\n",
    "    transform_list = []\n",
    "    if toTensor:\n",
    "        transform_list += [torchvision.transforms.ToTensor()]\n",
    "\n",
    "    if normalize:\n",
    "        transform_list += [torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                                                (0.26862954, 0.26130258, 0.27577711))]\n",
    "    return torchvision.transforms.Compose(transform_list)\n",
    "\n",
    "def numpy_to_pil(images):\n",
    "    \"\"\"\n",
    "    Convert a numpy image or a batch of images to a PIL image.\n",
    "    \"\"\"\n",
    "    if images.ndim == 3:\n",
    "        images = images[None, ...]\n",
    "    images = (images * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "    return pil_images\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def put_watermark(img, wm_encoder=None):\n",
    "    if wm_encoder is not None:\n",
    "        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "        img = wm_encoder.encode(img, 'dwtDct')\n",
    "        img = Image.fromarray(img[:, :, ::-1])\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_replacement(x):\n",
    "    try:\n",
    "        hwc = x.shape\n",
    "        y = Image.open(\"assets/rick.jpeg\").convert(\"RGB\").resize((hwc[1], hwc[0]))\n",
    "        y = (np.array(y)/255.0).astype(x.dtype)\n",
    "        assert y.shape == x.shape\n",
    "        return y\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "\n",
    "def check_safety(x_image):\n",
    "    safety_checker_input = safety_feature_extractor(numpy_to_pil(x_image), return_tensors=\"pt\")\n",
    "    x_checked_image, has_nsfw_concept = safety_checker(images=x_image, clip_input=safety_checker_input.pixel_values)\n",
    "    assert x_checked_image.shape[0] == len(has_nsfw_concept)\n",
    "    for i in range(len(has_nsfw_concept)):\n",
    "        if has_nsfw_concept[i]:\n",
    "            x_checked_image[i] = load_replacement(x_checked_image[i])\n",
    "    return x_checked_image, has_nsfw_concept\n",
    "\n",
    "def get_tensor(normalize=True, toTensor=True):\n",
    "    transform_list = []\n",
    "    if toTensor:\n",
    "        transform_list += [torchvision.transforms.ToTensor()]\n",
    "\n",
    "    if normalize:\n",
    "        transform_list += [torchvision.transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                (0.5, 0.5, 0.5))]\n",
    "    return torchvision.transforms.Compose(transform_list)\n",
    "\n",
    "def get_tensor_clip(normalize=True, toTensor=True):\n",
    "    transform_list = []\n",
    "    if toTensor:\n",
    "        transform_list += [torchvision.transforms.ToTensor()]\n",
    "\n",
    "    if normalize:\n",
    "        transform_list += [torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                                                (0.26862954, 0.26130258, 0.27577711))]\n",
    "    return torchvision.transforms.Compose(transform_list)\n",
    "\n",
    "\n",
    "def main_paint_by_example(img_p=None, ref_p=None, mask=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--outdir\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        help=\"dir to write results to\",\n",
    "        default=\"outputs/txt2img-samples\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--skip_grid\",\n",
    "        action='store_true',\n",
    "        help=\"do not save a grid, only individual samples. Helpful when evaluating lots of samples\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--skip_save\",\n",
    "        action='store_true',\n",
    "        help=\"do not save individual samples. For speed measurements.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ddim_steps\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"number of ddim sampling steps\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--plms\",\n",
    "        action='store_true',\n",
    "        help=\"use plms sampling\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fixed_code\",\n",
    "        action='store_true',\n",
    "        help=\"if enabled, uses the same starting code across samples \",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ddim_eta\",\n",
    "        type=float,\n",
    "        default=0.0,\n",
    "        help=\"ddim eta (eta=0.0 corresponds to deterministic sampling\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_iter\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"sample this often\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--H\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"image height, in pixel space\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--W\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"image width, in pixel space\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_imgs\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=\"image width, in pixel space\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--C\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"latent channels\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--f\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"downsampling factor\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_samples\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"how many samples to produce for each given reference image. A.k.a. batch size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_rows\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"rows in the grid (default: n_samples)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--scale\",\n",
    "        type=float,\n",
    "        default=5,\n",
    "        help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        type=str,\n",
    "        default=\"/home/ganimh/llm-grounded-diffusion/paint_by_example/configs/v1.yaml\",\n",
    "        help=\"path to config which constructs model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ckpt\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"path to checkpoint of model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=42,\n",
    "        help=\"the seed (for reproducible sampling)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--precision\",\n",
    "        type=str,\n",
    "        help=\"evaluate at this precision\",\n",
    "        choices=[\"full\", \"autocast\"],\n",
    "        default=\"autocast\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_path\",\n",
    "        type=str,\n",
    "        help=\"evaluate at this precision\",\n",
    "        default=\"/home/ganimh/llm-grounded-diffusion/paint_by_example/examples/image/example_1.png\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mask_path\",\n",
    "        type=str,\n",
    "        help=\"evaluate at this precision\",\n",
    "        default=\"/home/ganimh/llm-grounded-diffusion/paint_by_example/examples/examples/mask/example_1.png\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--reference_path\",\n",
    "        type=str,\n",
    "        help=\"evaluate at this precision\",\n",
    "        default=\"/home/ganimh/llm-grounded-diffusion/paint_by_example/examples/reference/example_1.jpg\"\n",
    "    )\n",
    "    opt = parser.parse_args(\"\")\n",
    "\n",
    "\n",
    "    seed_everything(opt.seed)\n",
    "\n",
    "    config = OmegaConf.load(f\"{opt.config}\")\n",
    "    model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    if opt.plms:\n",
    "        sampler = PLMSSampler(model)\n",
    "    else:\n",
    "        sampler = DDIMSampler(model)\n",
    "\n",
    "    os.makedirs(opt.outdir, exist_ok=True)\n",
    "    outpath = opt.outdir\n",
    "\n",
    "\n",
    "    batch_size = opt.n_samples\n",
    "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "\n",
    "    sample_path = os.path.join(outpath, \"source\")\n",
    "    result_path = os.path.join(outpath, \"results\")\n",
    "    grid_path=os.path.join(outpath, \"grid\")\n",
    "    os.makedirs(sample_path, exist_ok=True)\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    os.makedirs(grid_path, exist_ok=True)\n",
    "  \n",
    "\n",
    "    start_code = None\n",
    "    if opt.fixed_code:\n",
    "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
    "\n",
    "    precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                filename=os.path.basename(opt.image_path)\n",
    "                img_p = Image.open(opt.image_path).convert(\"RGB\")\n",
    "                image_tensor = get_tensor()(img_p)\n",
    "                image_tensor = image_tensor.unsqueeze(0)\n",
    "                ref_p = Image.open(opt.reference_path).convert(\"RGB\").resize((224,224))\n",
    "                ref_tensor=get_tensor_clip()(ref_p)\n",
    "                ref_tensor = ref_tensor.unsqueeze(0)\n",
    "                mask=Image.open(opt.mask_path).convert(\"L\")\n",
    "                mask = np.array(mask)[None,None]\n",
    "                mask = 1 - mask.astype(np.float32)/255.0\n",
    "                mask[mask < 0.5] = 0\n",
    "                mask[mask >= 0.5] = 1\n",
    "                mask_tensor = torch.from_numpy(mask)\n",
    "                inpaint_image = image_tensor*mask_tensor\n",
    "                test_model_kwargs={}\n",
    "                test_model_kwargs['inpaint_mask']=mask_tensor.to(device)\n",
    "                test_model_kwargs['inpaint_image']=inpaint_image.to(device)\n",
    "                ref_tensor=ref_tensor.to(device)\n",
    "                uc = None\n",
    "                if opt.scale != 1.0:\n",
    "                    uc = model.learnable_vector\n",
    "                c = model.get_learned_conditioning(ref_tensor.to(torch.float16))\n",
    "                c = model.proj_out(c)\n",
    "                inpaint_mask=test_model_kwargs['inpaint_mask']\n",
    "                z_inpaint = model.encode_first_stage(test_model_kwargs['inpaint_image'])\n",
    "                z_inpaint = model.get_first_stage_encoding(z_inpaint).detach()\n",
    "                test_model_kwargs['inpaint_image']=z_inpaint\n",
    "                test_model_kwargs['inpaint_mask']=Resize([z_inpaint.shape[-2],z_inpaint.shape[-1]])(test_model_kwargs['inpaint_mask'])\n",
    "\n",
    "                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "                samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
    "                                                    conditioning=c,\n",
    "                                                    batch_size=opt.n_samples,\n",
    "                                                    shape=shape,\n",
    "                                                    verbose=False,\n",
    "                                                    unconditional_guidance_scale=opt.scale,\n",
    "                                                    unconditional_conditioning=uc,\n",
    "                                                    eta=opt.ddim_eta,\n",
    "                                                    x_T=start_code,\n",
    "                                                    test_model_kwargs=test_model_kwargs)\n",
    "\n",
    "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                x_samples_ddim = x_samples_ddim.cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "                x_checked_image, has_nsfw_concept = check_safety(x_samples_ddim)\n",
    "                x_checked_image=x_samples_ddim\n",
    "                x_checked_image_torch = torch.from_numpy(x_checked_image).permute(0, 3, 1, 2)\n",
    "\n",
    "                def un_norm(x):\n",
    "                    return (x+1.0)/2.0\n",
    "                def un_norm_clip(x):\n",
    "                    x[0,:,:] = x[0,:,:] * 0.26862954 + 0.48145466\n",
    "                    x[1,:,:] = x[1,:,:] * 0.26130258 + 0.4578275\n",
    "                    x[2,:,:] = x[2,:,:] * 0.27577711 + 0.40821073\n",
    "                    return x\n",
    "\n",
    "                if not opt.skip_save:\n",
    "                    for i,x_sample in enumerate(x_checked_image_torch):\n",
    "                        \n",
    "\n",
    "                        all_img=[]\n",
    "                        all_img.append(un_norm(image_tensor[i]).cpu())\n",
    "                        all_img.append(un_norm(inpaint_image[i]).cpu())\n",
    "                        ref_img=ref_tensor\n",
    "                        ref_img=Resize([opt.H, opt.W])(ref_img)\n",
    "                        all_img.append(un_norm_clip(ref_img[i]).cpu())\n",
    "                        all_img.append(x_sample)\n",
    "                        grid = torch.stack(all_img, 0)\n",
    "                        grid = make_grid(grid)\n",
    "                        grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                        img = Image.fromarray(grid.astype(np.uint8))\n",
    "                        #img = put_watermark(img, wm_encoder)\n",
    "                        img.save(os.path.join(grid_path, 'grid-'+filename[:-4]+'_'+str(opt.seed)+'.png'))\n",
    "                        \n",
    "\n",
    "\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                        img = put_watermark(img, wm_encoder)\n",
    "                        img.save(os.path.join(result_path, filename[:-4]+'_'+str(opt.seed)+\".png\"))\n",
    "                        \n",
    "                        mask_save=255.*rearrange(un_norm(inpaint_mask[i]).cpu(), 'c h w -> h w c').numpy()\n",
    "                        mask_save= cv2.cvtColor(mask_save,cv2.COLOR_GRAY2RGB)\n",
    "                        mask_save = Image.fromarray(mask_save.astype(np.uint8))\n",
    "                        mask_save.save(os.path.join(sample_path, filename[:-4]+'_'+str(opt.seed)+\"_mask.png\"))\n",
    "                        GT_img=255.*rearrange(all_img[0], 'c h w -> h w c').numpy()\n",
    "                        GT_img = Image.fromarray(GT_img.astype(np.uint8))\n",
    "                        GT_img.save(os.path.join(sample_path, filename[:-4]+'_'+str(opt.seed)+\"_GT.png\"))\n",
    "                        inpaint_img=255.*rearrange(all_img[1], 'c h w -> h w c').numpy()\n",
    "                        inpaint_img = Image.fromarray(inpaint_img.astype(np.uint8))\n",
    "                        inpaint_img.save(os.path.join(sample_path, filename[:-4]+'_'+str(opt.seed)+\"_inpaint.png\"))\n",
    "                        ref_img=255.*rearrange(all_img[2], 'c h w -> h w c').numpy()\n",
    "                        ref_img = Image.fromarray(ref_img.astype(np.uint8))\n",
    "                        ref_img.save(os.path.join(sample_path, filename[:-4]+'_'+str(opt.seed)+\"_ref.png\"))\n",
    "\n",
    "    print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "          f\" \\nEnjoy.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97ed81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef479d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layoutgpt",
   "language": "python",
   "name": "layoutgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
